---
title: "SOC-GA 2332 Intro to Stats Lab 7"
author: "Wenhao Jiang"
date: "10/20/2023"
output:
  html_document:
    df_print: paged
    theme: paper
    highlight: textmate
---


<style type="text/css">

body{ 

    font-size: 18px;
    line-height: 1.7em;
    <!-- text-align: justify; -->

}

blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 18px;
    border: solid 1px;
    color: #008080 
}

h1 { font-size: 36px; }

h2 { font-size: 28px; }

h3 { font-size: 23px; }

</style>

<br>

---

## Housekeeping

- **Problem Set 2** is due on Friday Oct 23nd, 11:59 pm
- For Exam 3: Make sure that you review **(1) interactions with dummy variables, (2) different types of multivariate relationships, (3) interpretation of regression coefficients**  
- Replication Project Part 1 reminder: 

|         Task                       |  Timeline          |
|------------------------------------|--------------------|
| **Obtaining the raw data from IPUMS**  | **By Oct. 18th**       |
| Cleaning the data                  | Oct. 21th to Oct. 25nd   |
| Replicating Table A1a, Table A1b, and Figure 1 and put in LaTeX | Oct. 28th to Oct. 31th  |
| Replicating the regression results (Table A2a, Table A2b) and put in LaTeX | Nov. 1st to Nov. 5th |
| Write the 2-page project report + Wiggle room for formatting and debugging, etc. | Nov. 8th to Nov. 10th |  

- Note: You don't need to wait until the PS2 due day to submit a screenshot of your data cart. You can email me the screenshot once you are done so that I can help check if you have chosen the correct variables.
- There are multiple ways to include a figure in Rmarkdown. Please see one example below.

---

## Part 0: How to Include a Figure in RMarkdown

- We first import necessary packages for this tutorial.

```{r setup, message=FALSE, warning=FALSE}
library(pacman)
p_load(tidyverse, stargazer, kableExtra, gridExtra, 
       QuantPsyc, coefplot, sandwich, lmtest, ggthemes,
       ggpubr)
```

- We use `knitr` and `include_graphics()` to include figures.

```{r include figure, fig.cap="Types of Multivariate Relationships", out.width = '65%', fig.align='center'}
knitr::include_graphics("graph/multivariate_types.png")
```

---

## Part 1: How Does Multivariate Relationships Affect Regression Estimates

- Multiple Causes

```{r multiple causes, warning=FALSE, message=FALSE}
set.seed(2023)

## empty results
woz <- c()
wz <- c()

for (i in 1:1000){
 ## create hypothetical variables
  X <- runif(500, min=1, max=10)
  Z <- runif(500, min=2, max=5)
  Y <- 10 + 5*X + Z + rnorm(500,0,1)
  
  ## data
  data <- data.frame(X=X,Y=Y,Z=Z)
  
  ## regression
  lm1 <- lm(Y ~ X, data)
  lm2 <- lm(Y ~ X + Z, data) 
  
  ## extract results
  woz <- c(woz, summary(lm1)$coef[2,1])
  wz <- c(wz, summary(lm2)$coef[2,1])
}

## combine results
results <-
  data.frame(estimate = c(woz,wz),
             category = c(rep("Z excluded",1000),rep("Z included",1000)))

mean <- results %>%
  group_by(category) %>%
  summarize(mean = mean(estimate))

## plot
results %>%
  ggplot(aes(x=estimate,group=category)) +
  geom_density(aes(color=category),bw=0.01) +
  geom_vline(data = mean, aes(xintercept = mean, color = category)) +
  scale_color_manual(values=c("#999999", "red3")) +
  xlab("Estimate of X's slope, True = 5") +
  theme_bw()
```

- Confounding

```{r confounding, warning=FALSE, message=FALSE}
set.seed(2023)

## empty results
woz <- c()
wz <- c()

for (i in 1:1000){
  
 ## create hypothetical variables
  Z <- runif(500, min=1, max=10)
  X <- runif(500, min=1, max=5) + 0.5*Z
  Y <- 10 + 5*X + Z + rnorm(500,0,1)
  
  ## data
  data <- data.frame(X=X,Y=Y,Z=Z)
  
  ## regression
  lm1 <- lm(Y ~ X, data)
  lm2 <- lm(Y ~ X + Z, data) 
  
  ## extract results
  woz <- c(woz, summary(lm1)$coef[2,1])
  wz <- c(wz, summary(lm2)$coef[2,1])
}

## combine results
results <-
  data.frame(estimate = c(woz,wz),
             category = c(rep("Z excluded",1000),rep("Z included",1000)))

mean <- results %>%
  group_by(category) %>%
  summarize(mean = mean(estimate))

## plot
results %>%
  ggplot(aes(x=estimate,group=category)) +
  geom_density(aes(color=category),bw=0.01) +
  geom_vline(data = mean, aes(xintercept = mean, color = category)) +
  scale_color_manual(values=c("#999999", "red3")) +
  xlab("Estimate of X's slope, True = 5") +
  theme_bw()
```

- Mediation

```{r mediation, warning=FALSE, message=FALSE}
set.seed(2023)

## empty results
woz <- c()
wz <- c()

for (i in 1:1000){
  
 ## create hypothetical variables
  X <- runif(500, min=1, max=10)
  Z <- runif(500, min=2, max=5) + 0.5*X
  Y <- 2*Z + rnorm(500,0,1)
  
  ## data
  data <- data.frame(X=X,Y=Y,Z=Z)
  
  ## regression
  lm1 <- lm(Y ~ X, data)
  lm2 <- lm(Y ~ X + Z, data) 
  
  ## extract results
  woz <- c(woz, summary(lm1)$coef[2,1])
  wz <- c(wz, summary(lm2)$coef[2,1])
}

## combine results
results <-
  data.frame(estimate = c(woz,wz),
             category = c(rep("Z excluded",1000),rep("Z included",1000)))

mean <- results %>%
  group_by(category) %>%
  summarize(mean = mean(estimate))

## plot
results %>%
  ggplot(aes(x=estimate,group=category)) +
  geom_density(aes(color=category),bw=0.01) +
  geom_vline(data = mean, aes(xintercept = mean, color = category)) +
  scale_color_manual(values=c("#999999", "red3")) +
  theme_bw()
```

- Moderation

---

## Part 2: Additional Topics in Regression

### 1. Heteroskedasticity and Robust Standard Errors

- Heteroskedasticity occurs when the **variance of the error term changes across different values of the explanatory variables**; $Var(\epsilon_i | X) \neq Var(\epsilon_i)$, or, as we see in lecture, we assume that $Var(\epsilon_i | X) = \sigma^2 h(X)$
- Heteroskedasticity violates the basic assumption of OLS, in which the variance of the error term should be constant across different values of the explanatory variables. 
- Will heteroskedasticity make estimates biased and inconsistent?
- In OLS estimation, the standard error of $\hat{\beta}_1$, $se_{\hat{\beta}_1}$ is derived by assuming homoskedasticity. Specifically, given known $X$ and the uncertainty coming from sampling the same $X$ but with different $\epsilon_i$ from the population, we assume $Var(y|X) = Var(\epsilon|X) = Var(\epsilon) = \sigma^2$

$$
\begin{aligned}
\hat{\mathbf{\beta}} &= (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{y} \\
\textrm{Var}(\hat{\mathbf{\beta}}|\mathbf{X}) &= (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime}
  \; \textrm{Var}(\mathbf{y}|\mathbf{X}) \; \mathbf{X}  (\mathbf{X}^{\prime} \mathbf{X})^{-1} \\
 &= (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime}
 \;\sigma^2 \mathbf{I} \; \mathbf{X}  (\mathbf{X}^{\prime} \mathbf{X})^{-1} \\
 &= \sigma^2 (\mathbf{X}^{\prime} \mathbf{X})^{-1} (\mathbf{X}^{\prime}
 \mathbf{X})  (\mathbf{X}^{\prime} \mathbf{X})^{-1} \\
 &= \sigma^2  (\mathbf{X}^{\prime} \mathbf{X})^{-1}
\end{aligned}
$$

```{r heteroskedasticity, warning=FALSE, message=FALSE}
set.seed(2023)

## empty results
homo <- c()
hetero <- c()

## create x
X <- rgamma(5000, 5, 4)

for (i in 1:1000){
  
 ## create hypothetical data
  homo_Y <- -0.25 + 1.2*X + rnorm(5000,0,1)
  hetero_Y <- -0.25 + 1.2*X + rnorm(5000,0,0.5*X)
  
  ## data
  homo_data <- data.frame(X=X,homo_Y)
  hetero_data <- data.frame(X=X,hetero_Y)
  
  ## regression
  lm1 <- lm(homo_Y ~ X, homo_data)
  lm2 <- lm(hetero_Y ~ X, hetero_data)
  
  ## extract results
  homo <- c(homo, summary(lm1)$coef[2,1])
  hetero <- c(hetero, summary(lm2)$coef[2,1])
}

## combine results
results <-
  data.frame(estimate = c(homo,hetero),
             category = c(rep("homoskedasticity",5000),rep("heteroskedasticity",5000)))

## plot
hetero <-
  results %>%
  filter(category=="heteroskedasticity") %>%
  ggplot(aes(x=estimate)) +
  geom_histogram(aes(y=..density..),fill="grey",color="black") +
  stat_function(fun = dnorm, 
                args = list(mean = 1.2, 
                            sd = summary(lm2)$coef[2,2]),
                color = "red3") +
  ggtitle("Estimate under heteroskedasticity") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

homo <-
  results %>%
  filter(category=="homoskedasticity") %>%
  ggplot(aes(x=estimate)) +
  geom_histogram(aes(y=..density..),fill="grey",color="black") +
  stat_function(fun = dnorm, 
                args = list(mean = 1.2, 
                            sd = summary(lm1)$coef[2,2]),
                color = "red3") +
  ggtitle("Estimate under Homoskedasticity") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

ggarrange(hetero, homo)

```

- Detecting heteroskedasticity by plotting $Y - \hat{Y}$ over $X$

```{r detect hetero, warning=FALSE, message=FALSE}
data.frame(X = X, e = hetero_Y - predict.lm(lm2,hetero_data)) %>%
  ggplot(aes(x=X,y=e)) +
  geom_point(color="red3",alpha=0.4) +
  theme_bw()

```

- Robust standard error
- If you have a reason to believe that your dataset violates the assumption of homoskedasticity, you can use the packages `sandwich` and `lmtest` to get robust standard errors. 

```{r robust se, warning=FALSE, message=FALSE}

## original SE
summary(lm2)$coef

## robust SE
coeftest(lm2, vcov = vcovHC(lm2, type="HC1"))

results %>%
  filter(category=="heteroskedasticity") %>%
  ggplot(aes(x=estimate)) +
  geom_histogram(aes(y=..density..),fill="grey",color="black") +
  stat_function(fun = dnorm, 
                args = list(mean = 1.2, 
                            sd = coeftest(lm2, vcov = vcovHC(lm2, type="HC1"))[2,2]),
                color = "red3") +
  ggtitle("Estimate Uncertainty under heteroskedasticity and Robust SE") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```

### 2. Clustering of Errors

- One of the basic OLS assumptions is that the error term is independently distributed across observations. i.e.: $$\text{Corr}(\epsilon_i, \epsilon_j | X) = 0 \:\: \forall \:\: i\neq j$$

- This assumption could be violated when your data have a "nested" structure, or your data is ordered by time and the trend is highly correlated between time unit. In such cases, you should employ other modeling techniques to address correlated errors. For example, you can use multilevel modeling for nested data, and longitudinal data analysis techniques for time-series data.

### 3. F-test for Nested Models

Import the `earnings_df` data (the one we used for lab 5 and 6) and estimate models:

```{r import and model}

# Load cleaned and recoded df
load("data/earnings_df.RData")

# Examine data
head(earnings_df, 10) %>% kbl("html") %>% kable_classic_2(full_width = F)

# Estimate Nested Models
m1 <- lm(earn ~ age_recode, 
         data = earnings_df)

m2 <- lm(earn ~ age_recode + edu,
         data = earnings_df)

m3 <- lm(earn ~ age_recode + edu + female,
         data = earnings_df)

m4 <- lm(earn ~ age_recode + edu + female + black + other,
         data = earnings_df)

m5 <- lm(earn ~ age_recode + edu + female + black + other + edu*female,
         data = earnings_df)

# Examine models
stargazer(m1, m2, m3, m4, m5, 
          type="text", 
          omit.stat=c("ser", "f"),
          star.char = c("*", "**", "***"),
          star.cutoffs = c(0.05, 0.01, 0.001))

```


- We can use F-test to compare two regression models. The idea behind the F-test for nested models is to check **how much errors are reduced after adding additional predictors**.  A relatively large reduction in error yields a large F-test statistic and a small P-value. The P-value for F statistics is the right-tail probability.  

- If the F's p-value is significant (smaller than 0.05 for most social science studies), it means that at least one of the additional $\beta_j$ in the full model is not equal to zero.  
  
- The F test statistic for nested regression models is calculated by:

$$F = \frac{(SSE_\text{restricted} - SSE_\text{full})/df_1}{SSE_\text{full}/df_2} $$
where $df_1$ is the number of **additional** predictors added in the full model and $df_2$ is the **residual df for the full model**, which equals $(n - 1 - \text{number of IVs in the complete model})$. The $df$ of the F test statistic is $(df_1, df_2)$.  

For example, according to the equation, we can hand-calculate the F value for `m3` vs `m4`:

```{r F-hand}

# SSE_restricted:
sse_m3 <- sum(m3$residuals^2)

# SSE_full:
sse_m4 <- sum(m4$residuals^2)

# We add one additional IV, so:
df1 <- 2

# Residual df for the full model (m5):
df2 <- m4$df.residual

# Calculate F:
F_stats <- ((sse_m3 - sse_m4)/df1)/(sse_m4/df2)
F_stats

# Check tail probability using `1 - pf()`
1 - pf(F_stats, df1, df2) 
```
  
- You can also use `anova()` to perform a F-test in R. 

```{r anova}

anova(m3, m4)

```
  
- What is your null and alternative hypotheses? What's your decision given the F-test result?  

---

### 4. Standardized Regression Coefficients

- Why sometimes people report standardized regression coefficients? As we covered in the lecture, the size of a regression coefficient depends on **the scale at which the independent and dependent variables are measured**. 

- For example, assume that in a regression model the coefficient of population on the national GDP is 0.0001. This means that 1 additional person will lead to 0.0001 increase in the GDP. However, this value does not necessarily imply that the effect of population is less pronounced than other predictors whose coefficients have a larger value. Because the value of the coefficient depends on the measurement unit of the IV. If we now change population to **population in million**, the new coefficient of population will become $0.0001 \cdot 10^6 = 100$. Although the value of the coefficient gets much larger, this increase is caused by a change in the measurement unit, not the actual effect of population. 

- Therefore, it is problematic to use the raw value of the regression coefficient as indicators of relative effect size if the predictors in the model have different measurement units. In such scenarios, standardized regression coefficients can help compare the relative effect size of the predictors even if they are measured in different units. 

- Standardized coefficients convert both your dependent variable and independent variables to **z-scores**. That is, each of your original (numeric) variables are converted to have a mean of 0 and a standard deviation of 1. Thus, **standardized coefficients tell us the change in $Y$, in $Y$'s standard deviation units, for a one-standard-deviation increase in $X_i$, while holding other $X$s constant**.  

- There are two methods of getting standardized regression coefficients in R.

#### Method 1: Use `lm.beta()` from the `QuantPsyc` package

You can get standardized regression coefficients by using the `lm.beta()` function in the `QuantPsyc` package. For example, if we want to get the standardized coefficients for Model 2 (`earn ~ age_recode + edu`):

```{r lmbeta}
# Original model
m2

# Standardized coefficients
std_m2 <- lm.beta(m2)
std_m2
```

  
- But this method will only report the point estimates instead of a comprehensive modeling result. To obtain that, we need to convert all numeric variables to z-scores and estimate regression models based on the transformed data.

#### Method 2: Create Z-scores for All Numeric Variables

- For each numeric variables, we create the "standardized variables" by calculating their z-scores: 

$$z = \frac{x - \bar{x}}{s_x}$$
  
- For example, we can use `mutate_at()` to covert numeric variables to z-scores in `earnings_df` using the above formula:
  
```{r z-standardize}

# A function that convert a numeric vector to a z-score vector
get_zscore <- function(x){
  (x - mean(x, na.rm = T))/sd(x, na.rm = T)
  }

# Create a df with numeric variables converted to z-score
earnings_df_std <- earnings_df %>%
  mutate_at(c("edu", "age_recode", "earn"), get_zscore)

# Examine data
head(earnings_df_std, 10) %>% kbl("html") %>% kable_classic_2(full_width = F)

# Estimate model
m2_std_zscore <-  lm(earn ~ age_recode + edu, data = earnings_df_std)

# Compare results
stargazer(m2, m2_std_zscore, type = "text")

```
