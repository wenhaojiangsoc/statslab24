---
title: "SOC-GA 2332 Intro to Stats Lab 4 Exercise Solution"
author: "Di Zhou"
date: "9/24/2021"
output:
  html_document:
    df_print: paged
    theme: paper
    highlight: textmate
    toc: true
  pdf_document: 
    toc: true
---


<style type="text/css">

body{ 

    font-size: 16px;
    line-height: 1.7em;
    <!-- text-align: justify; -->

}

blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 16px;
    border: solid 1px;
}

h1 { font-size: 32px; }

h2 { font-size: 24px; }

h3 { font-size: 20px; }

</style>

<br>

---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(kableExtra)
library(ipumsr)

```

### Part 1 Exercise: Calculate OLS estimators by hand

Let us apply the formulas we learned in the lecture to work out the value of $\hat{\beta_0}$ and $\hat{\beta_1}$ in the above toy example using OLS.  

1. Hand-calculate the value of $\hat{\beta_0}$ and $\hat{\beta_1}$ using the formulas below. 

>$$\hat{\beta_1} = \frac{\sum^{n}_{i=1}(x_i - \overline x)(y_i - \overline y)}{\sum^{n}_{i=1}(x_i - \overline x)^2}$$
$$\hat{\beta_0} = \overline y - \hat{\beta_1}\overline x$$

  
$$\bar x = \frac{17}{5} = 3.4, \text{   and   }\bar y = \frac{62}{5} = 12.4$$  
$$\sum^{n}_{i=1}(x_i - \overline x)^2 = (-2 - 3.4)^2 + (0 - 3.4)^2 + \dots + (10 - 3.4)^2 = 91.2$$
$$\sum^{n}_{i=1}(x_i - \overline x)(y_i - \overline y) = (-2 - 3.4)(-1 - 12.4) + \dots + (10 - 3.4)(28 - 12.4) = 188.2$$
$$\hat{\beta_1} = \frac{188.2}{91.2} = 2.06$$
$$\hat{\beta_0} = \overline y - \hat{\beta_1}\overline x = 12.4 - 2.06 *3.4 = 5.40$$

2. Now create the toy example dataframe `ols_df` in your R environment using the code above. Then calculate $\hat{\beta_0}$ and $\hat{\beta_1}$ using R. *Hint:* You can perform vector operations in R. 

```{r ols_df}

# Value of x and y are observed
x <- c(-2, 0, 3, 6, 10) 
y <- c(-1, 8, 15, 12, 28)

# Create a dataframe of x and y
ols_df <- tibble(
  x = x,
  y = y
)

```

```{r part1-q2}

# Calculate the mean of x and y
x_mean = mean(ols_df$x)
y_mean = mean(ols_df$y)

# beta1 & beta0 according to the formula in Q1
beta1 = sum((x - x_mean)*(y -y_mean))/sum((x-x_mean)^2)
beta0 = y_mean - beta1*x_mean

# check results
beta0
beta1

```
  
3. In R, you can get the variance of a value vector using the function `var()`, and the covariance of two value vectors using the function `cov()`. Applying these R functions to calculate $\hat\beta_1$ using the following formula:

>$$\hat{\beta_1} = \frac{\sum^{n}_{i=1}(x_i - \overline x)(y_i - \overline y)}{\sum^{n}_{i=1}(x_i - \overline x)^2} = \frac{Cov(X, Y)}{Var(X)}$$

```{r part1-q3}

# This should equal to beta1 you got in Q1 & Q2
cov(ols_df$x, ols_df$y)/var(ols_df$x) 

```
 
4. You can use the `cor()` function in R to calculate the correlation coefficient $\rho$ of two value vectors. Now calculate $\hat\beta_1$ using the formula below.

>$$\hat{\beta_1} = \frac{\sum^{n}_{i=1}(x_i - \overline x)(y_i - \overline y)}{\sum^{n}_{i=1}(x_i - \overline x)^2} = \frac{Cov(X, Y)}{Var(X)} = \rho_{X, Y} \cdot \frac{\sigma_Y}{\sigma_X}$$
 
```{r part1-q4}

# Calculate the correlation rho
rho = cor(ols_df$x, ols_df$y)
# Calculate sigma_x and sigma_y, which is the standard deviation of x and y
sigma_x = sd(ols_df$x)
sigma_y = sd(ols_df$y)

# This should equal to beta1 you got in Q1-Q3
rho*sigma_y/sigma_x

```
  
5. Create a new dataframe based on `ols_df` that has a new variable called `fitted_y` that equals to your predicted value of y given your OLS regression equation. *Hint:* Use `mutate()` in `tidyverse` to create a new variable.
  
```{r part1-q5}

# Use mutate() to add the variable
ols_df_fit <- ols_df %>%
  mutate(fitted_y = beta0 + beta1*x)

ols_df_fit
```

6. Calculate your OLS regression's **SSE (sum of squared errors)**. *Hint:*You can either hand-code based on the formula, or start by creating a new variable that gives you $\epsilon_i^2$ and then work out the SSE. Remember, given $Y = \beta_0 + \beta_1X + \epsilon$, we have $\epsilon_i = y_i - (\beta_0 + \beta_1x_i)$.

> $$SSE = \sum^{n}_{i=1}\epsilon_i^2 = \sum^{n}_{i=1}[y_i - (\hat{\beta_0} + \hat{\beta_1} x_i)]^2 $$

```{r part1-q6}

# Handcode the formula
(ols_df$y - (beta0 + beta1*ols_df$x))^2 %>% sum()
# #quivalently
(ols_df$y - ols_df_fit$fitted_y)^2 %>% sum()


# Use the dataframe

# Create a variable equal to the squared error (y - fitted_y) of each row
ols_df_fit %>% 
  mutate(
    error = y - fitted_y, # you don't need to create this, this is for reference
    squared_error = (y - fitted_y)^2
    ) 

# Use the summarise() to calculate the sum of squared errors
ols_df_fit %>% 
  mutate(squared_error = (y - fitted_y)^2) %>%
  summarise(sse = sum(squared_error))

```

7. Draw your OLS regression line on the scatter plot created earlier. *Hint:* You can add a line with customized intercept and slope value using the `geom_abline()` function:

```
your_plot +
  geom_abline(intercept = your_intercept, slope = your_slope)

```
```{r part1-q7}

ols_df %>%
  ggplot() +
  geom_point(aes(x, y), shape = 1) +
  labs(title = "Scatterplot of Example Data with Fitted OLS Line") +
  geom_abline(intercept = beta0, slope = beta1) # plug in your beta results

```

---

### Part 2 Exercise 1: Download Data from IPUMS

7. Load the data to your R environment, by using the code shown in the webpage when you click the "R" Command File link in your IPUMS data downloading page. 

<p align="center">
![](graph/rcommand_demo.png){width=60%}
</p>

<p align="center">
![](graph/r_command_copy.png){width=60%}
</p>

```{r part2-exercise1-ipums}

# Load ipums data
ddi <- read_ipums_ddi("data/usa_00009.xml") # content inside the quote depends on your file name & path
data <- read_ipums_micro(ddi)

```

---

### Part 2 Exercise 2: Read Data Codebook

Going through the variable description, and answer the following question:

1. What does the variable "PERNUM" represents? How can you uniquely identify each person within the IPUMS with this variable?  
PERNUM is "Person number in sample unit." We can identify each individuals within IPUMS when we combine PERNUM with SAMPLE and SERIAL.

2. What does the variable "PERWT" represents? When should you consider using this variable?   
PERWT is "Person weight". It is generally a good idea to use PERWT when conducting a person-level analysis of any IPUMS sample. 

3. For the variable "SEX", what are the possible values it can take? What does each value represent? Try run `str(data$SEX)`, what do you see?   
1 and 2  
1 = Male, 2 = Female. 

4. What does the variable "EDUC" represents? How many values this variable can take? What is the value that represents N/A?   
EDUC is "Educational attainment [general version]". It can take values from 0 to 11. Value 0 represents N/A or no schooling. 

5. For the variable "INCWAGE", what are the codes for N/A and missing data?   
999999 = N/A  
999998 = Missing

---

### Part 3 Exercise: Estimate OLS Model Using IPUMS Data
  
1. Clean your data using the code above (create `unique_id` and remove missing values for INCWAGE and EDUC)
```{r part3-exercise-q1}

# Select variables
data_clean <- data %>%
  select(SAMPLE, SERIAL, PERNUM, PERWT, SEX, EDUC, INCWAGE)


# Create a new variable called "unique_id"
data_clean <- data_clean %>%
  unite("unique_id",                # The name of the new column, as a string or symbol
        SAMPLE, SERIAL, PERNUM,     # Columns to unite
        sep = "",                   # Separator to use between values
        remove = TRUE)              # Remove input columns from output data frame

# Remove missing value use filter()
data_clean <- data_clean %>%
  filter(EDUC != 0 & INCWAGE < 999998)
# 4610 obs removed, 17322 obs left

```

2. Plot a scatter plot with EDUC on the x axis and INCWAGE on the y axis
```{r part3-exercise-q2}

data_clean %>%
  ggplot() +
  geom_point(aes(EDUC, INCWAGE)) +
  labs(title = "Scatterplot of Education and Wage Income",
       subtitle = "NA and missing values are removed, n = 17,322")

```

3. Use `lm()` to fit a OLS regression model with INCWAGE as the dependent variable and EDUC as the independent variable. Report your $\hat{\beta_0}$, $\hat{\beta_1}$, and SSE.

```{r part3-exercise-q3}

# fit a model using edu to predict wage
ols_ipums <- lm(INCWAGE ~ EDUC, data = data_clean)
summary(ols_ipums)

# Beta0:
ols_ipums$coefficients[1]

# Beta1:
ols_ipums$coefficients[2]

# SSE
sum(ols_ipums$residuals^2)

# If you want to display the digit instead of the scientific notion
# You can set this option
options(scipen = 999)
sum(ols_ipums$residuals^2)

```

4. Use `+ geom_smooth(aes(x = your_x, y = your_y), method = "lm")` to plot the fitted regression line on top of your scatter plot. 

```{r part3-exercise-q4}

data_clean %>%
  ggplot() +
  geom_point(aes(EDUC, INCWAGE)) +
  geom_smooth(aes(EDUC, INCWAGE), method = "lm") +
  labs(title = "Scatterplot of Education and Wage Income with Fitted OLS Regression")

```
5. Plot the following diagnostic plots. What patterns do you see about the residual terms relative to the value of EDUC and fitted y? Do you think the data and the model set up fulfill the Gauss-Markov assumptions?  

*Hint:* Use the base R function `plot(x = your_x_vector, y = your_y_vector)` for quick plotting.  

- $\epsilon_i$ vs $x_i$: Make a scatter plot with the predictor on the x axis and the model residual on the y axis.  

**The scatter plot suggests that we may have violated the assumption of homoskedasticity because the variance of the model residuals does not look constant across the value of x. As level of education gets larger, the residuals get more spread out. In addition, it suggests that as the level of education gets higher the model tend to understimate one's wage. This may suggest a nonlinear relationship between X and Y.** 

```{r }

plot(x = data_clean$EDUC, y = ols_ipums$residuals)

```

- $\epsilon_i$ vs $\hat y_i$: Make a scatter plot with the predicted y on the x axis and the model residual on the y axis.  

**Similar conclusions can be draw with this plot. Note that in bivariate regression, these two plots have direct relationship ($\hat y = \beta_0 + \beta_1x_i$). However, when we move to multivariate regression, regression with interactions, and nonlinear regression, these two plots won't have such a relationship, therefore you need to check both types of plots to make sure the residual terms behave well according to the OLS assumptions.** 

```{r }

plot(x = ols_ipums$fitted.values, y = ols_ipums$residuals)

```

---
